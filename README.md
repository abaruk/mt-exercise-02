# MT Exercise 2

## Data
For Task 1, I used the WikiText-2 dataset provided in the repository under:  
`tools/pytorch-examples/word_language_model/data/wikitext-2`

No additional preprocessing was needed since the data was already tokenized and properly formatted.

---

## Training

The model was trained using the script:

```bash
./scripts/train.sh


